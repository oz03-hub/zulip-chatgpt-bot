{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 token = ~4 chars,\n",
    "1 token = ~3/4 word,\n",
    "100 tokens = ~75 words\n",
    "\n",
    "Token usage can change based on which chat model we use, for example:\n",
    "\n",
    "- gpt-3.5-turbo: TPM=90k\n",
    "- gpt-3.5-turbo-16k-0613: TPM=180k\n",
    "\n",
    "> TPM: Tokens per minute\n",
    "\n",
    "Token limit for models is the threshold where the prompt and completion must be in the set boundaries. For example gpt-3.5-turbo token limit is 4096, so if we had a prompt that had 4000 tokens, the completion from the model would be 96 tokens at most etc.\n",
    "\n",
    "- Low Usage: In low-usage scenarios, users tend to interact minimally with a language model. This could involve short queries or commands, such as asking simple questions or issuing basic instructions. In terms of token count, low usage could range from around 10 to 100 tokens per interaction.\n",
    "\n",
    "- Medium Usage: Medium-usage scenarios involve more involved conversations or requests, where users engage in longer interactions with the language model. This could include discussing a topic in more depth, providing additional context, or having a back-and-forth conversation. In terms of token count, medium usage might range from approximately 100 to 500 tokens per interaction.\n",
    "\n",
    "- High Usage: High-usage scenarios involve extensively utilizing the language model, such as conducting lengthy conversations, generating multiple paragraphs of text, or making complex queries that require substantial context and detailed responses. In terms of token count, high usage could range from around 500 to several thousand tokens per interaction.\n",
    "\n",
    "> Tokenizer of openai can be found [here](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import re\n",
    "import openai\n",
    "import zulip\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import sqlite3\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key=os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_THRES = 100\n",
    "MED_THRES = 500\n",
    "CONVO_THRES = MED_THRES #Threshold to limit conversation history\n",
    "\n",
    "delimiter = \"###\"\n",
    "\n",
    "def get_completion_with_tokens(messages, prompt, model='gpt-3.5-turbo'): #tuple: str, dict: {\"prompt_tokens\": #, \"completion_tokens\": #, \"total_tokens\": #}\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    messages.append(response.choices[0].message)\n",
    "    return response.choices[0].message[\"content\"], response.usage\n",
    "\n",
    "def get_completion(messages, prompt, model='gpt-3.5-turbo'): #str\n",
    "    r, _ = get_completion_with_tokens(messages, prompt, model)\n",
    "    return r\n",
    "\n",
    "def count_token_history(messages, model='gpt-3.5-turbo'): #int\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    tokens = 0\n",
    "    for message in messages: #role: ###, content: ###\n",
    "        tokens += len(encoding.encode(message[\"content\"]))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def usage_level(tokens):\n",
    "    if tokens < LOW_THRES:\n",
    "        return \"LOW\"\n",
    "    elif tokens < MED_THRES:\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        return \"HIGH\"\n",
    "\n",
    "def sum_history(history, model='gpt-3.5-turbo'):\n",
    "    text = \"\"\n",
    "    for m in history[:-2]:\n",
    "        text += m[\"content\"] + \" \"\n",
    "    \n",
    "    text = text.rstrip()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following text delimited by {delimiter}. \\\n",
    "    Keep your summarization short, but do not loose important information. \\\n",
    "    {delimiter}{text}{delimiter}\n",
    "    \"\"\"\n",
    "    \n",
    "    nhistory = [get_completion([], prompt, model)]\n",
    "    nhistory.extend(history[:-2])\n",
    "    return nhistory\n",
    "\n",
    "def handle_new(messageHistory, nPrompt, model='gpt-3.5-turbo'):\n",
    "    response, usage = get_completion_with_tokens(messages=messageHistory, prompt=nPrompt, model=model)\n",
    "    \n",
    "    if usage[\"total_tokens\"] > CONVO_THRES:\n",
    "        messageHistory = sum_history(messageHistory, model)\n",
    "    \n",
    "    return response, messageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [{\"role\": \"system\", \"content\": \"You are a friendly chatbot that can hold conversations for a long period and summarizes message history efficiently.\"}]\n",
    "\n",
    "def print_history(history):\n",
    "    for h in history:\n",
    "        print(\"{}: {}\".format(h[\"role\"], h[\"content\"]))\n",
    "\n",
    "user_in = input(\"--> \")\n",
    "while user_in != \"quit\":\n",
    "    if user_in == \"history\":\n",
    "        print_history(history)\n",
    "    else:\n",
    "        response, history = handle_new(history, user_in)\n",
    "        print(\"AI: {}\".format(response))\n",
    "    user_in = input(\"--> \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
